{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient method with Armijo Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\"><img src=\"globalized_bfgs_method.png\" width=\"700\" height=\"600\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Size: Wolfe-Powell Rule\n",
    "- For an unconstrained minimization problem, the Wolfe Powell rule are a set of inequalities that perform inexact line search.\n",
    "- This allows for reduction the objective function sufficiently rather than exactly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\"><img src=\"wolfe_powell.png\" width=\"700\" height=\"600\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rosenbrock Function\n",
    "- Introduced by Howard H. Rosenbrock in 1960, used as a performance test problem for optimization problems.\n",
    "- The Rosenbrock function $r: \\mathbb{R}^2 \\rightarrow \\mathbb{R}$ is given by:\n",
    "$$r(x) = 100 (x_2 - x_1^2)^2+ (1 - x_1)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from src.function import Function\n",
    "\n",
    "class Rosenbrock(Function):\n",
    "\n",
    "    def eval(self, x):\n",
    "        assert len(x) == 2, '2 dimensional input only.'\n",
    "        return 100 * (x[1] - x[0] ** 2) ** 2 + (1 - x[0]) ** 2\n",
    "\n",
    "    def gradient(self, x):\n",
    "        assert len(x) == 2, '2 dimensional input only.'\n",
    "        return np.array([\n",
    "            2 * (-200 * x[0] * x[1] + 200 * np.power(x[0], 3) - 1 + x[0]),\n",
    "            200 * (x[1] - x[0] ** 2)\n",
    "        ])\n",
    "\n",
    "    def hessian(self, x):\n",
    "        assert len(x) == 2, '2 dimensional input only.'\n",
    "        df_dx1 = -400 * x[1] + 1200 * x[0] ** 2 + 2\n",
    "        df_dx1dx2 = -400 * x[0]\n",
    "        df_dx2dx1 = -400 * x[0]\n",
    "        df_dx2 = 200\n",
    "\n",
    "        return np.array([[df_dx1, df_dx1dx2], [df_dx2dx1, df_dx2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bateman Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from src.function import Function\n",
    "\n",
    "T = np.array([15, 25, 35, 45, 55, 65, 75, 85, 105, 185, 245, 305, 365])\n",
    "Y = np.array([0.038, 0.085, 0.1, 0.103, 0.093, 0.095, 0.088, 0.08, 0.073, 0.05, 0.038, 0.028, 0.02])\n",
    "\n",
    "\n",
    "class Bateman(Function):\n",
    "\n",
    "    def eval(self, x, t_list=T):\n",
    "        y_x_t = x[2] * (np.exp(-x[0] * t_list) - np.exp(-x[1] * t_list))\n",
    "        f_raw = np.power(y_x_t - Y, 2)\n",
    "\n",
    "        return 0.5 * np.sum(f_raw)\n",
    "\n",
    "    def gradient(self, x):\n",
    "        y_x_t = x[2] * (np.exp(-x[0] * T) - np.exp(-x[1] * T))\n",
    "\n",
    "        dx1 = 0\n",
    "        dx2 = 0\n",
    "        dx3 = 0\n",
    "        for i in range(13):\n",
    "            dx1 = dx1 + (x[2] * -T[i] * (np.exp(-x[0] * T[i]))) * (y_x_t[i] - Y[i])\n",
    "            dx2 = dx2 + (x[2] * T[i] * np.exp(-x[1] * T[i])) * (y_x_t[i] - Y[i])\n",
    "            dx3 = dx3 + (np.exp(-x[0] * T[i]) - np.exp(-x[1] * T[i])) * (y_x_t[i] - Y[i])\n",
    "\n",
    "        return np.array([dx1, dx2, dx3])\n",
    "\n",
    "    def hessian(self, x):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "- The parameters will be the following:\n",
    "$$\\beta := 0.5, \\sigma := 10^{-4}, \\varepsilon := 10^{-4}$$\n",
    "- Start point will be the following:\n",
    "$$x^0 := (-1.2, 1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Point: [1.         0.99999999]\n",
      "Iterations: 34\n"
     ]
    }
   ],
   "source": [
    "from src.optimizers.bfgs_method import BFGSMethod\n",
    "\n",
    "objective = Rosenbrock()\n",
    "starting_point = np.array([-1.2, 1])\n",
    "H_0 = np.array([[1, 0],\n",
    "                [0, 1]])\n",
    "\n",
    "rho = 0.9\n",
    "sigma = 1e-4\n",
    "epsilon = 1e-6\n",
    "\n",
    "optimizer = BFGSMethod()\n",
    "x = optimizer.optimize(starting_point,\n",
    "                       H_0,\n",
    "                       rho,\n",
    "                       sigma,\n",
    "                       epsilon,\n",
    "                       objective)\n",
    "\n",
    "print(f'Optimal Point: {x}')\n",
    "print(f'Iterations: {optimizer.iterations}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
